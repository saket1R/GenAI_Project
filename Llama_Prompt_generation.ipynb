{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:58:54.729531Z","iopub.execute_input":"2024-11-17T03:58:54.729891Z","iopub.status.idle":"2024-11-17T03:58:55.098688Z","shell.execute_reply.started":"2024-11-17T03:58:54.729856Z","shell.execute_reply":"2024-11-17T03:58:55.097769Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/leftover/ashish_news/cleaned_processed_MARUTI_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\n/kaggle/input/leftover/ashish_news/cleaned_processed_NTPC_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\n/kaggle/input/leftover/ashish_news/cleaned_processed_SBILIFE_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\n/kaggle/input/leftover/ashish_news/cleaned_processed_ONGC_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\n/kaggle/input/leftover/ashish_news/cleaned_processed_SBIN_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\n/kaggle/input/leftover/ashish_news/cleaned_processed_RELIANCE_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\n/kaggle/input/leftover/ashish_news/cleaned_processed_SHRIRAMFIN_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\n/kaggle/input/leftover/ashish_news/cleaned_processed_TATACONSUM_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\n/kaggle/input/leftover/ashish_news/cleaned_processed_POWERGRID_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl\n%pip install -U datasets\n%pip install -U wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:06:29.854120Z","iopub.execute_input":"2024-11-17T04:06:29.855071Z","iopub.status.idle":"2024-11-17T04:07:51.301776Z","shell.execute_reply.started":"2024-11-17T04:06:29.855024Z","shell.execute_reply":"2024-11-17T04:07:51.300688Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport subprocess\n\n# Check if CUDA is available\nif not torch.cuda.is_available():\n    print(\"CUDA is not available. Attempting to fix...\")\n\n    # List available GPUs\n    gpu_info = subprocess.run(\"nvidia-smi\", shell=True, capture_output=True, text=True)\n    if \"NVIDIA\" in gpu_info.stdout:\n        print(\"GPU detected:\")\n        print(gpu_info.stdout)\n    else:\n        print(\"No GPU found. Please ensure the Kaggle runtime is set to use a GPU.\")\n    \n    # Check CUDA version compatibility with installed PyTorch\n    print(f\"PyTorch version: {torch.__version__}\")\n    print(\"CUDA availability in PyTorch:\", torch.cuda.is_available())\nelse:\n    print(\"CUDA is available and ready for use.\")\n    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:08:18.013738Z","iopub.execute_input":"2024-11-17T04:08:18.014124Z","iopub.status.idle":"2024-11-17T04:08:21.779313Z","shell.execute_reply.started":"2024-11-17T04:08:18.014085Z","shell.execute_reply":"2024-11-17T04:08:21.778330Z"}},"outputs":[{"name":"stdout","text":"CUDA is available and ready for use.\nDevice Name: Tesla T4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom huggingface_hub import login\nfrom peft import PeftModel\nimport torch\nfrom trl import SFTTrainer, setup_chat_format\n\n# Define torch_dtype\ntorch_dtype = torch.float16\nattn_implementation = \"eager\"\n\n# QLoRA config with BitsAndBytes\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Log in to Hugging Face with your API token\nlogin(\"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\")  # Replace with your actual Hugging Face token\n\n# Set the base model\nbase_model = \"meta-llama/Llama-3.1-8B-Instruct\"  # The pre-existing Hugging Face model repository\n\n# Load the tokenizer from Hugging Face Hub\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\n# Load the model with quantization config\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,  # path to the pre-existing model\n    quantization_config=bnb_config,\n    device_map=\"auto\",  # Automatically use CUDA if available or fallback to CPU\n)\n\n# Check if the tokenizer already has a chat template\nif tokenizer.chat_template is not None:\n    tokenizer.chat_template = None  # Remove the existing chat template if you want to overwrite it\n\n# Modify the model and tokenizer setup\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\n# Resize token embeddings to match the tokenizer\nmodel.resize_token_embeddings(len(tokenizer))\n\n# If you're using LoRA, load LoRA configuration (optional, remove if not using LoRA)\n# model = PeftModel.from_pretrained(model, \"your-lora-model\")  # Add LoRA model here if needed\n\n# Your further processing code goes here...\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:08:22.890528Z","iopub.execute_input":"2024-11-17T04:08:22.891035Z","iopub.status.idle":"2024-11-17T04:16:52.263926Z","shell.execute_reply.started":"2024-11-17T04:08:22.890998Z","shell.execute_reply":"2024-11-17T04:16:52.262913Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051cef24a0ef45e68954f9f77a8477e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33763d5601ba44bfab1c964d6aeb1de0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d95c971723d418eacacc3d9aaba5bed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6cfd854ad2d4dee948c0996ed7b5957"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6718815544245bab764890b03c0b0f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e15b41c03d5341baaed4defcee66ff32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70604c5f2854d4c9e2bebbc3eee6010"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34759824cf87436e99c40e3192a0f558"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b94baf60bf14ed79642048372b20c34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b755ae295d5f458592c8be636170aee4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4141257adb474f168b9b500e3e4d4d60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2d3814dac844a569d62f59f955ca063"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Embedding(128258, 4096)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\n\n# Ensure the tokenizer is set correctly for left-padding\ntokenizer.padding_side = \"left\"\n\n# Define the paths for the Kaggle input and output directories\nINPUT_DIR = \"/kaggle/input/leftover/ashish_news\"  # Replace with your actual dataset folder\nOUTPUT_DIR = \"/kaggle/working/predictions\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Define the task prompt (remains static)\ntask_prompt = \"\"\"\nBased on the provided information, provide a **specific quantitative prediction** of the stock movement for the next week.\n\nYour response must:\n1. Provide a specific stock movement prediction within a numerical range (e.g., \"up by 2-4%\" or \"down by 1-3%\").\n2. Summarize the reasoning for the prediction based on the company's performance, market trends, and external factors.\n\nFormat your response as:\nPrediction: The stock is expected to [rise/fall] by [X-Y]% next week due to [reasoning].\n\"\"\"\n\n# Function to generate predictions\ndef generate_prediction(prompt):\n    input_to_model = prompt.strip() + \"\\n\\n\" + task_prompt.strip()\n\n    # Tokenize and prepare input for the model\n    inputs = tokenizer(input_to_model, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n\n    # Generate output using the model\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            repetition_penalty=1.2,\n        )\n\n    # Decode the generated output\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Extract the response: Remove prompt and task prompt\n    response_output = generated_text.replace(prompt.strip(), \"\").replace(task_prompt.strip(), \"\").strip()\n\n    # Clean the response to ensure only prediction and reasoning remain\n    lines = response_output.split(\"\\n\")\n    cleaned_response = \"\\n\".join(line for line in lines if \"Prediction:\" in line or \"due to\" in line)\n\n    return cleaned_response\n\n# Process all CSV files in the input directory\nfor filename in os.listdir(INPUT_DIR):\n    if filename.endswith(\".csv\"):\n        # Load the CSV file\n        file_path = os.path.join(INPUT_DIR, filename)\n        df = pd.read_csv(file_path)\n\n        # Ensure the dataset has a column with the prompt\n        if 'prompt' not in df.columns:\n            print(f\"Skipping {filename}: No 'prompt' column found.\")\n            continue\n\n        # Apply the prediction function to each row\n        print(f\"Processing file: {filename}\")\n        df['prediction'] = df['prompt'].apply(generate_prediction)\n\n        # Save the updated DataFrame to the output directory\n        output_path = os.path.join(OUTPUT_DIR, f\"predictions_{filename}\")\n        df.to_csv(output_path, index=False)\n        print(f\"Saved predictions to {output_path}\")\n\nprint(\"All files processed and predictions saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:19:48.339238Z","iopub.execute_input":"2024-11-17T04:19:48.340024Z","iopub.status.idle":"2024-11-17T06:40:05.778987Z","shell.execute_reply.started":"2024-11-17T04:19:48.339984Z","shell.execute_reply":"2024-11-17T06:40:05.778052Z"}},"outputs":[{"name":"stdout","text":"Processing file: cleaned_processed_MARUTI_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nSaved predictions to /kaggle/working/predictions/predictions_cleaned_processed_MARUTI_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nProcessing file: cleaned_processed_NTPC_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nSaved predictions to /kaggle/working/predictions/predictions_cleaned_processed_NTPC_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nProcessing file: cleaned_processed_SBILIFE_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nSaved predictions to /kaggle/working/predictions/predictions_cleaned_processed_SBILIFE_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nProcessing file: cleaned_processed_ONGC_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nSaved predictions to /kaggle/working/predictions/predictions_cleaned_processed_ONGC_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nProcessing file: cleaned_processed_SBIN_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nSaved predictions to /kaggle/working/predictions/predictions_cleaned_processed_SBIN_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nProcessing file: cleaned_processed_RELIANCE_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nSaved predictions to /kaggle/working/predictions/predictions_cleaned_processed_RELIANCE_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nProcessing file: cleaned_processed_SHRIRAMFIN_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nSaved predictions to /kaggle/working/predictions/predictions_cleaned_processed_SHRIRAMFIN_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nProcessing file: cleaned_processed_TATACONSUM_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nSaved predictions to /kaggle/working/predictions/predictions_cleaned_processed_TATACONSUM_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nProcessing file: cleaned_processed_POWERGRID_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nSaved predictions to /kaggle/working/predictions/predictions_cleaned_processed_POWERGRID_2023-01-01_2023-11-30_nobasics_llama-3.1.csv\nAll files processed and predictions saved.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\n\n# Ensure the tokenizer is set correctly for left-padding\ntokenizer.padding_side = \"left\"\n\n# Define the news content (input) and the task prompt separately\nnews_content = \"\"\"\nAdani Enterprises Limited is a leading entity in the Thermal Coal sector. Incorporated and publicly traded since N/A, the company has established its reputation as one of the key players in the market. As of today, Adani Enterprises Limited has a market capitalization of 3601.49 in INR, with 1140.00 shares outstanding.\n\nAdani Enterprises Limited operates primarily in the India, trading under the ticker ADANIENT.NS on the NSI. As a dominant force in the Thermal Coal space, the company continues to innovate and drive progress within the industry.\n\nFrom 2023-01-08 to 2023-01-15, ADANIENT.NS's stock price decreased from 3818.89 to 3715.93. Company news during this period includes:\n\n1. The company is planning to raise ₹120,000 crore ($2.5 billion) through an FPO.\n2. Fluctuations in related sectors such as fertilizers and power indicate market volatility.\n\nKey observations include increasing investor interest, sector-specific challenges, and recent stock price fluctuations.\n\"\"\"\n\ntask_prompt = \"\"\"\nBased on the provided information, provide a **specific quantitative prediction** of the stock movement for the next week.\n\nYour response must:\n1. Provide a specific stock movement prediction within a numerical range (e.g., \"up by 2-4%\" or \"down by 1-3%\").\n2. Summarize the reasoning for the prediction based on the company's performance, market trends, and external factors.\n\nFormat your response as:\nPrediction: The stock is expected to [rise/fall] by [X-Y]% next week due to [reasoning].\n\"\"\"\n\n# Combine the news content and task prompt for input\ninput_to_model = news_content.strip() + \"\\n\\n\" + task_prompt.strip()\n\n# Tokenize and prepare input for the model\ninputs = tokenizer(input_to_model, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n\n# Generate output using the model with adjusted parameters\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=200,  # Ensure enough space for a complete response\n        temperature=0.7,     # Control randomness\n        top_p=0.9,           # Focus coherence\n        do_sample=True,      # Introduce variability\n        repetition_penalty=1.2  # Penalize repetitive sequences\n    )\n\n# Decode the generated output\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract the response: Remove news content and task prompt\nresponse_output = generated_text.replace(news_content.strip(), \"\").replace(task_prompt.strip(), \"\").strip()\n\n# Further clean the response to ensure only prediction and reasoning remain\nlines = response_output.split(\"\\n\")\ncleaned_response = \"\\n\".join(line for line in lines if \"Prediction:\" in line or \"due to\" in line)\n\n# Print the clean response\nprint(\"Generated Response:\\n\")\nprint(cleaned_response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T11:22:49.389456Z","iopub.execute_input":"2024-11-16T11:22:49.389882Z","iopub.status.idle":"2024-11-16T11:23:07.763047Z","shell.execute_reply.started":"2024-11-16T11:22:49.389841Z","shell.execute_reply":"2024-11-16T11:23:07.762123Z"}},"outputs":[{"name":"stdout","text":"Generated Response:\n\n(Note: Please follow these instructions strictly.)  |  Prediction: The stock is expected to fall by 1.7-3.6% next week due to market volatility and recent stock price fluctuations following announcements like raising funds via an IPO amidst other developments that may impact confidence levels among investors.    |\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\n\n# Ensure the tokenizer is set correctly for left-padding\ntokenizer.padding_side = \"left\"\n\n# Define the news content (input) and the task prompt separately\nnews_content = \"\"\"\n[Company Introduction]:\n\nAdani Enterprises Limited is a leading entity in the Thermal Coal sector. Incorporated and publicly traded since N/A, the company has established its reputation as one of the key players in the market. As of today, Adani Enterprises Limited has a market capitalization of 3601.49 in INR, with 1140.00 shares outstanding.\n\nAdani Enterprises Limited operates primarily in the India, trading under the ticker ADANIENT.NS on the NSI. As a dominant force in the Thermal Coal space, the company continues to innovate and drive progress within the industry.\n\nFrom 2023-01-08 to 2023-01-15, ADANIENT.NS's stock price decreased from 3818.89 to 3715.93. Company news during this period are listed below:\n\n[Headline]: International Holding Company, Gulf SWFs may go big on Adani Enterprises’ $2.5 billion FPO\n[Summary]: Adani Enterprises is the flagship business entity of the conglomerate. It will raise as much as ₹120,000 crore ($2.5 billion) from retail and institutional investors. This will be the largest such issuance in India to date.\n\n[Headline]: Stock market update: Fertilisers stocks up as market rises\n[Summary]: The NSE Nifty50 index was trading 37.9 points up at 17952.05. National Fertilizer (down 0.91%), Mangalore Chemicals & Fertilizers (down 0.47%), Coromandel International (down 0.37%), Agro Phos (down 0.35%) and Madras Fertilizers (down 0.12%) were among the top gainers.\n\n[Headline]: Stocks in news: Infosys, HCL Tech, Cyient, RIL, HUL, Tata Motors, Route Mobile\n[Summary]: The contract was trading at 18,000, up 50 points or 0.28% from the previous close. The company is expected to report a consolidated revenue growth of 3.2% sequentially in constant currency terms for the quarter.\n\n[Headline]: Stock market update: Power stocks down as market rises\n[Summary]: PIGL (down 3.40%), Voltamp Transformers (down 2.44%), JSW Energy (down 2.01%), Transformers and Rectifiers (India) (down 1.54%), CESC (down 1.53%), Alstom T&D India (down 1.41%), Hitachi Energy India (down 1.08%), Energy Development Company (down 1.06%), KEC International (down 0.92%) and Torrent Power (down 0.71%) were among the top gainers of the day.\n\n[Headline]: Share market update: Most active stocks of the day in terms of traded value\n[Summary]: RIL (₹2335.82 crore), Infosys (₹1737.42 crore), ICICI Bank (₹972.61 crore), Bharti Airtel (₹931.40 crore), HDFC (₹882.12 crore), HCL Tech (₹845.05 crore), Tata Steel (₹828.67 crore), HDFC Bank (₹738.94 crore) and Kotak Bank (R).\n\"\"\"\n\ntask_prompt = \"\"\"\nAnalyze the key positive and negative factors for the given company, identifying 2-4 critical factors based on all available information. Based on your analysis, provide a specific, quantitative prediction of the stock movement for the next week as a percentage change within a precise range (e.g., up by 2-4% or down by 1-3%). Ensure the output is grounded in logical reasoning and critical evaluation of the identified factors.\n\nYour output must include:\n1. A summary of key positive and negative factors.\n2. A specific numerical range for the predicted stock movement.\n3. The reasoning behind your quantitative prediction.\n\"\"\"\n\n# Combine the news content and task prompt\ninput_to_model = news_content + \"\\n\\n\" + task_prompt\n\n# Tokenize and prepare input for the model\ninputs = tokenizer(input_to_model, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n\n# Generate output using the model with adjusted parameters\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=300,  # Increased to allow complete response\n        temperature=0.7,     # Balanced response diversity\n        top_p=0.9,           # Nucleus sampling for coherence\n        do_sample=True,      # Sampling for variability\n        repetition_penalty=1.2  # Penalize repetitive sequences\n    )\n\n# Decode the generated output\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Remove input content from the generated output\nresponse_output = generated_text.replace(input_to_model.strip(), \"\").strip()\n\n# Print only the model's generated response\nprint(\"Generated Response:\\n\")\nprint(response_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T10:21:02.214909Z","iopub.execute_input":"2024-11-16T10:21:02.215684Z","iopub.status.idle":"2024-11-16T10:21:29.462253Z","shell.execute_reply.started":"2024-11-16T10:21:02.215645Z","shell.execute_reply":"2024-11-16T10:21:29.461323Z"}},"outputs":[{"name":"stdout","text":"Generated Response:\n\nBased on my thorough analysis of the provided data, here’s an attempt to address each requested aspect:\n\n### Summary of Key Positive and Negative Factors\n\n**Positive Factors**\n\n*   **Strong Market Performance**: Despite some fluctuations, the overall trend suggests that the Indian economy remains robust, which can positively impact companies like Adani Enterprises.\n*   **Significant Fundraising Efforts**: With plans to raise up to ₹120,000 crores through a Follow-on Public Offer (FPO), there could be increased investor confidence and liquidity support.\n*   **Diversified Business Portfolio**: Being part of a large conglomerate gives it access to various sectors and resources, potentially mitigating risks associated with any single segment.\n\n\n**Negative Factors**\n\n*   **Market Volatility**: The recent decrease from 3818.89 to 3715.93 indicates volatility, suggesting potential short-term challenges despite long-term prospects.\n*   **Sector-Specific Challenges**: While diversified, performance in certain segments might not align perfectly with broader market trends or global economic shifts.\n\n\n\nGiven these considerations, let us proceed with predicting the future stock movement.\n\n### Predicted Stock Movement Range\n\n\nConsidering both positive and negative aspects, I predict a moderate increase due to anticipated improvements in market conditions and fundraising efforts outweighing current minor setbacks.\n\nOver the next week (from 2023-01-22 to 2023-01-29), considering historical patterns, fundamental analysis, and external influences on the financial markets, we anticipate a slight\n","output_type":"stream"}],"execution_count":11}]}